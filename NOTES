<!-- -*- mode: markdown; -*- -->

Cache für einzelne Linien innerhalb eines Stripes, d.h. chunk_size *
length(stripe) große Linien.  Wenn eine Anfrage kommt, die kleiner ist, als
diese Größe, wird zuerst eine Linie von den Speichermedien geladen und
anschließend mittels dieser Daten die Anfrage bearbeitet (entweder von dort
gelesen, oder mit den gelesenen Daten kombiniert und zurückgeschrieben).

Alle Anfragen müssen kleiner gleich einer Cacheline sein (was über die
Einstellungen der Anfragequeue sichergestellt wird).

Ähnliche Funktionalität ist in raid5.c vorhanden.

cache_line_write_back(cache_line)
sector_to_cache_line(cache_line) : sector_t

struct cache_line {
       /* lock must be hold when data in this line is manipulated */
       lock_t lock;
       /* corresponding stripe */
       raidxor_stripe_t *stripe;
       /* the index of this cache line from the beginning of the stripe */
       unsigned long index;
       /* either DIRTY, WRITEBACK or CLEAN */
       unsigned int status;
       /* the number of allocated pages (same as the number of units in
          the stripe */
       unsigned long length;
       struct *page[0];
};

Eine Cacheline hat entweder den Status CLEAN oder DIRTY.  Wenn eine
Leseanfrage hereinkommt, deren Offset in eine geladenen Cacheline fällt, wird
die Anfrage aus dem Cache bearbeitet, ansonsten wird sie nachgeladen.

Wenn der Cache voll ist, aber eine neue Cacheline geladen werden muss, werden
Lines mit CLEAN nur gelöscht, solche mit DIRTY zuerst zurückgeschrieben.
Während des Zurückschreibens wird die Cacheline mit WRITEBACK markiert.

D.h. alle Anfragen gehen zuerst an den Cachelayer, der nur große Blöcke an die
darunterliegenden Geräte zulässt.  Der Cache sorgt dafür, das die Daten immer
gültig sind und verhandelt die Kommunikation mit den Speichergeräten.

Der Zugriff auf einzelne Cachelines ist nur über den Lock des Caches möglich.

Es wäre möglich, für den Zugriff auf den gesamten Cache (Line einfügen, Line
löschen) und den Zugriff auf einzelne Lines (Daten schreiben, Daten lesen)
einen Read-Write-Lock für den Cache einzusetzen, da (vermutlich) das Einfügen
und Löschen von Lines seltener passiert, als Daten innerhalb einer Line
bearbeitet werden.
Plan
----
- raid{0,1,5}.c zusammenkürzen und ausdrucken
  - welche Funktionen werden von md.c aufgerufen?

- Verstehen von `md.c`, `raid0.c`, `raid1.c` und `raid5.c`
- Modul in `raidxor.c`
- Implementierung Deviceerstellung
- Implementierung Benutzerwerkzeuge
  - Festlegung von Konfigurationsformat
- Implementierung allgemeine XOR-Gleichungen (zuerst unabhängig vom
  Kernelmodul)
- Testfälle
  - Normal Lesen/Schreiben
  - Ausgefallene Platten / Fehlerfälle
  - Konkurrierende Zugriffe
- Implementierung Lesezugriff
- Implementierung Schreibzugriff
- Korrektheit
- Performanceanalyse

- Übertragung der Dekodierungsgleichungen über Userspacedaemon auf
  Anfrage.
- D.h. erstmal nur Kodierung und Dekodierung ohne Fehlerkorrektur.

Sekundär:
- zusätzliche Parallelisierung mit Kernelthreads



+----------+ +----------+
|Resource 1| |Resource 2|
+----------+ +----------+
|  Red 1   | |  Unit 4  |
+==========+=+==========+==========+
¦  Unit 2  ¦ ¦  Red 2   ¦ Stripe 2 ¦
+==========+=+==========+==========+
|  Red 3   | |  Unit 6  |
+----------+ +----------+

Das heisst, ein Stripe besteht aus N Laufwerken, die entweder Units
oder Redundancy sind.  Die Reihenfolge ist egal.  Die Redundancies
sind beim Schreiben wichtig und beim Lesen mit Fehlerfall.

Wenn eine Einheit ausfällt, dann für immer.  Die Wiederherstellung
muss über einen Userspacedaemon oder interne Berechnung der
Dekodierungsgleichungen geschehen.

Die Blockgrösse wird von mdadm vorgegeben: Potenzen von 2 grösser als
512 und kleiner als 4096 (?).  Eine Funktion muss eine Anfrage an das
Raid aufspalten mit diesem Parameter; eine zweite sollte einen Satz
fertiger Übertragungen wieder zusammensetzen (ebenfalls mit diesem
Parameter).



XOR mit `xor_blocks` in `xor.h`.

http://www.geocities.com/ravikiran_uvs/articles/blkdevarch.html

Zur struct bio: http://lwn.net/Articles/26404/, vor allem bio_clone im
letzten Absatz.  Auch http://lwn.net/Articles/27055/ und
http://lwn.net/Articles/27361/:

> If the make request function can arrange for the transfer(s) described
> in the given bio, it should do so and return zero. "Stacking" drivers
> can also redirect the bio by changing its bi_bdev field and returning
> nonzero; in this case the bio will then be dispatched to the new
> device's driver (this is as things were done in 2.4). 
>
> If the "make request" function performs the transfer itself, it is
> responsible for passing the BIO to bio_endio() when the transfer is
> complete. Note that the "make request" function is not called with the
> queue lock held.

Ein sector sind 512 Bytes, also genau die Größe, die wir hier
verwenden werden.

<!--
Wie groß sind die bio_vec Blöcke?  Kann man sie auf Blockgröße (512
Bytes) begrenzen, sodass anschließend nur diese Blöcke umgehängt
werden müssen?  blk_queue_hardsect_size scheint da ein Punkt zu sein.
-->

Es folgt: `bio_vec`s sind beliebig groß und haben hintereinander, also
als Array innerhalb eines `bio`s, eine Größe vielfach zu 512 Byte.

Gibt es irgendwelche Einschränkungen für die `bio_vec`s?  Können wir
welche forcieren?

Die bisherigen Raidimplementierungen gruppieren darüber hinaus mehrere
Blöcke auf einem Device (auf dem Raid?) in Chunks (was mit Problemen
beim Zugriff über Chunkgrenzen hinweg führt).

Aus http://www.makelinux.net/ldd3/chp-16-sect-1.shtml:

> 16.1.4. A Note on Sector Sizes
>
> As we have mentioned before, the kernel treats every disk as a linear
> array of 512-byte sectors. Not all hardware uses that sector size,
> however. Getting a device with a different sector size to work is not
> particularly hard; it is just a matter of taking care of a few
> details. The sbull device exports a hardsect_size parameter that can
> be used to change the "hardware" sector size of the device; by looking
> at its implementation, you can see how to add this sort of support to
> your own drivers. 

> The first of those details is to inform the kernel of the sector size
> your device supports. The hardware sector size is a parameter in the
> request queue, rather than in the gendisk structure. This size is set
> with a call to blk_queue_hardsect_size immediately after the queue is
> allocated: 
>
> blk_queue_hardsect_size(dev->queue, hardsect_size);
>
> Once that is done, the kernel adheres to your device's hardware sector
> size. All I/O requests are properly aligned at the beginning of a
> hardware sector, and the length of each request is an integral number
> of sectors. You must remember, however, that the kernel always
> expresses itself in 512-byte sectors; thus, it is necessary to
> translate all sector numbers accordingly.

Ansonsten folgt daraus Umkopieren in Puffer für jedes einzelne
Speichergerät und Konstruktion von neuen bios.  Anschließend
Berechnung der Redundanzen und anschließend das Schreiben auf die
Platten.

Für die Puffer ist ein Cache ähnlich wie der stripe-cache von raid5.c
angebracht, der Aufbau muss das Einfügen von fehlenden Daten (zur
Berechnung der Redundanzen) unterstützen, d.h. solange noch Blöcke von
einer Platte fehlen, blockieren wir.  Welche Informationen sind für
diese Art Puffer noch notwendig?


Der `md`-Treiber
----------------
Der `md`-Treiber implementiert RAID-0,1,(4,5,6),10 in unterschiedlichen
Modulen.  Die geklammerte Liste ist das Modul in der Datei `raid5.c`.

Für RAID-6 existieren verschiedene Implementierungen abhängig von den
Fähigkeiten des Prozessors (SIMD-Unterstützung wie MMX, SSE).

- Was ist ein `path_selector`?
- Wie funktioniert das `dm-log`? (Ist das überhaupt wichtig?)
- Wo werden Fehler erkannt (und kategorisiert)?
  - Funktion `error` in `raid1.c:980`

RAID-1 ist als Startpunkt nützlich, dann RAID-0 und -(4,5,6).

Der `md`-Treiber hat in u.a. in raid5.c Funktionen für Striping.
- Wie funktioniert die?
  - Wo ist die Parität? ops_complete_postxor, ops_run_compute5,
    ops_complete_compute5

Kernelthreading wird benutzt, Funktion `md_thread` in `md.c:4833`,
`md_register_thread` in `md.c:4883`.  `md.c` definiert nur einen
Synchronisationsthread `md_do_sync` in `md.c:5421`, `raid1.c`
definiert einen Thread `raid1d` in `raid1.c:1504`, `raid5.c` definiert
einen Thread `raid5d` in `raid5.c:3913`.

> This is our raid5 kernel thread.
>
> We scan the hash table for stripes which can be handled now.
> During the scan, completed stripes are saved for us by the interrupt
> handler, so that they will not have to wait for our next wakeup.

`raid0.c` hat keinen eigenen Thread.

Code
----
- Dokumentation in welchem Format?
- Quelltextdokumentation in speziellem Format?
- Quelltextformatierung?

Architektur
-----------
Einklinken in den `md`-Treiber ist möglich, die verschiedenen Module
sind separat zuschaltbar (raid-1,5,6), also können wir auch ein
raid-xor mit beliebigem Layout erstellen.

Das Hinzufügen und Entfernen von Platten ist bei unserem System im
laufenden System nicht möglich (?!), siehe Methoden `hot_disk_add` und
`hot_disk_remove` von `mdk_personality`.

- Können wir die `mdk_personality` des `raid5` Moduls
  benutzen/abändern?  Das Level in der `mdk_personality`-Struktur
  führt zur Auswahl unterschiedlicher Algorithmen und
  Paritätsplatten.

  Es scheint so zu sein, als ob das Abändern des Moduls in Hinsicht
  auf die Verständlichkeit nicht zu empfehlen ist.  Demnach separates
  Modul `raidxor.c` o.ä.

  - Welche Funktionen wären unterschiedlich und müssten geändert
    werden?

- Wie wird die Konfiguration geladen?

  Integration mit mdadm, da das Format schon etwas ähnlich ist?
  Dementsprechend wäre auch das Anlegen und Öffnen von Raids
  geregelt.

  Ratsam wäre das `sysfs`, da dort devicespezifische Daten gelagert
  werden.

  `mdadm` lädt über `ioctl`s die Konfiguration und stößt anschließend
  die `run`-Methode an; wir können ebenfalls ein eigenes `ioctl`
  darüber legen, dafür müssen die `block_device_operations` für unsere
  Devices überschrieben werden (`raidxor_ioctl` usw.).

  - Wie sieht das `sysfs` von `md` aus?
  - Wie interagiert [`mdadm`][mdadm] mit dem `sysfs` / Kerneltreiber?
  - Blockdevice, dementsprechend nach /sys/block/md/raid-xor?

  [`Sysfs`][sysfs]:
  > For device drivers and devices, attributes may be created. These are
  > simple files; the rule is that they should only contain a single
  > value and/or allow a single value to be set (unlike some files in
  > procfs, which need to be heavily parsed). These files show up in the
  > subdirectory of the device driver respective to the device. Using
  > attribute groups, a subdirectory filled with attributes may also be
  > created.

  Also ein Attribut Kodierungsgleichungen, ein Attribut
  Dekodierungsgleichungen, ...

  - Eigentlich sollen Attribute ASCII kodiert sein - entweder die Regel
    brechen, oder Parser innerhalb des Kernels.

  - Was ist dann mit Schreibzugriff?
  - Wie wird ein Device erstellt und wie gelöscht?

  Ein Userspaceprogramm sollte die Vorverarbeitung der Daten tätigen,
  die dann binär in den Kernel geschrieben werden.

  Da die Gleichungen vergleichsweise groß sind, passen sie wohl nicht
  in den Superblock, also kann von einem derart kodierten RAID nicht
  gebootet werden (ist das überhaupt wichtig?).  Das heißt ohne
  jedesmal die Gleichungen neu zu laden ist kein automatisches Laden
  eines RAID-Verbundes möglich.

  - Es könnte die Möglichkeit geben, bestimmte Kodierungen in den
    Kernel zu kompilieren (separate Module?), die entsprechend
    ausgewählt werden können.  Es könnte dann die Möglichkeit geben,
    dieses Modul im Superblock zu referenzieren.

- Wenn mehrere Devices erstellt werden können, wie wird das behandelt?

  Da MD von selbst mehrere Devices unterstützt, muß der Treiber
  sowieso dementsprechend entwickelt werden.

- Zuordnung von Gleichungen/Einstellungen auf Devices?

  Jedes Device hat eine eigene Menge von Gleichungen.  Wenn diese
  Gleichungen als Modul geladen werden könnten, muß auf
  Speicherfreigabe geachtet werden.

- Parallele Bearbeitung?
  - über Kernelthreads
  - über SIMD

  Locking muss sowieso gemacht werden, dementsprechend wären nur
  andere De-/Kodierungsschemata zu wählen.

[sysfs]: http://en.wikipedia.org/wiki/Sysfs
[mdadm]: http://neil.brown.name/blog/mdadm

[superblocks]: http://linux-raid.osdl.org/index.php/RAID_superblock_formats
[raidsetup]: http://linux-raid.osdl.org/index.php/RAID_setup
[linuxjournal]: http://www.linuxjournal.com/article/2391 "The Linux
RAID-1, 4, 5 Code"
